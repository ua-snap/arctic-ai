{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import the functions and load the PDF documents. The documents will be stored as `langchain` objects in the \"documents\" class, and we will explore some of the properties of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from populate_database import *\n",
    "\n",
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the three PDFs have 16 pages, 116 pages, and 32 pages each. So we can check that the loader splits the document up into objects by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the metadata per object.\n",
    "\n",
    "We see a dictionary with the file name and the original page number. Note that page numbers are pythonic and start at 0, and will likely not conform to PDF document page numbers with prefix/appendix pages, etc.\n",
    "\n",
    "We could add anything we want to this dictionary, including URLs, authors, year of publication, etc. Later, when running the model and recieving responses with citations, we will see how adding more metadata would be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/Alaskas-Changing-Environment-2024.pdf', 'page': 10}\n",
      "{'source': 'data/Alaskas-Changing-Environment_2019_WEB.pdf', 'page': 0}\n",
      "{'source': 'data/ArcticReportCard_full_report2024.pdf', 'page': 115}\n"
     ]
    }
   ],
   "source": [
    "print(documents[10].metadata)\n",
    "print(documents[32].metadata)\n",
    "print(documents[-1].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to have a unique ID for document objects, but not required... here we didn't assign any and the attribute is blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[10].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the page content for the object. If you compare to the PDF report, you will see that this is all of the text on the entire page, even text associated with graphics. Here we can see that some of the information lacks context because we don't have any images or specific spatial arrangement of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "FebruaryOctober 15 May 15\n",
      "2021–2022 winter12 inches of snow each day\n",
      "0\n",
      "2022–2023 winter\n",
      "0\n",
      "12 inches of snow each day\n",
      "2023–2024 winter\n",
      "0\n",
      "12 inches of snow each day\n",
      "Spotlight event: Heavy snow\n",
      "Winter warming in recent decades has been \n",
      "significant, but so far, much of Alaska remains cold \n",
      "enough for most winter precipitation to fall as snow. \n",
      "With warmer ocean temperatures, more moisture \n",
      "is available to evaporate, and when the ingredients \n",
      "come together, heavy snowfalls can occur.\n",
      "In Anchorage, the past three winters all had one \n",
      "or more snowstorms producing more than a foot \n",
      "of snow in a 24-hour period. Following heavy snow \n",
      "in November 2023, numerous Anchorage streets \n",
      "went unplowed for days due to a mismatch between \n",
      "city and state road maintenance. Residents were \n",
      "stranded in homes, and businesses were unable to \n",
      "open without employees. In 2024, several dozen roofs \n",
      "collapsed and Anchorage officials warned more than \n",
      "1,000 commercial property owners that their roofs \n",
      "were at risk due to the heavy snow loads. Impacts \n",
      "from the storm were ongoing for months.\n",
      "ANCHORAGE SNOW\n",
      "Daily snow for the past three \n",
      "winters in Anchorage. The \n",
      "2023-24 season produced 133.3 \n",
      "inches of snow and was 0.2 \n",
      "inches shy of the 2011-12 record. \n",
      "11\n",
      "Loren Holmes, Anchorage Daily News 11\n"
     ]
    }
   ],
   "source": [
    "print(documents[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this format, these pages are simply too big to make use of. The embedding model needs to split the text into smaller chunks that can be tokenized. While the embedder we chose (`nomic-embed-text`) has a long context length of up to 8192 tokens, making it useful for long text blocks, we still want to chunk the text into smaller units to improve our search capabilities.\n",
    "\n",
    "For PDFs, we are using `langchain`'s `RecursiveCharacterTextSplitter` function, which attempts to keep larger units (e.g., paragraphs) intact. If a unit exceeds the chunk size, it moves to the next level (e.g., sentences). This is an example of [text-structure based splitting](https://python.langchain.com/docs/concepts/text_splitters/#text-structured-based), which uses natural hierarchical units of text (paragraphs, sentences, words) in order to split the text up.\n",
    "\n",
    "In other branches of this repo, we intend to explore [document-structure based splitting](https://python.langchain.com/docs/concepts/text_splitters/#document-structured-based) on structured files like Markdown, JSON, or HTML, which will use the features unique to those formats to provide the logical organization of the document used in splitting.\n",
    "\n",
    "For now, let's see the chunking when we use a limit of 800 characters with an 80 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(documents)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the chunks derived from the page content above. We can see that the page has been split, and that there is some overlap between the two chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/Alaskas-Changing-Environment-2024.pdf', 'page': 10}\n",
      "11\n",
      "FebruaryOctober 15 May 15\n",
      "2021–2022 winter12 inches of snow each day\n",
      "0\n",
      "2022–2023 winter\n",
      "0\n",
      "12 inches of snow each day\n",
      "2023–2024 winter\n",
      "0\n",
      "12 inches of snow each day\n",
      "Spotlight event: Heavy snow\n",
      "Winter warming in recent decades has been \n",
      "significant, but so far, much of Alaska remains cold \n",
      "enough for most winter precipitation to fall as snow. \n",
      "With warmer ocean temperatures, more moisture \n",
      "is available to evaporate, and when the ingredients \n",
      "come together, heavy snowfalls can occur.\n",
      "In Anchorage, the past three winters all had one \n",
      "or more snowstorms producing more than a foot \n",
      "of snow in a 24-hour period. Following heavy snow \n",
      "in November 2023, numerous Anchorage streets \n",
      "went unplowed for days due to a mismatch between \n",
      "city and state road maintenance. Residents were\n",
      "\n",
      "\n",
      "{'source': 'data/Alaskas-Changing-Environment-2024.pdf', 'page': 10}\n",
      "city and state road maintenance. Residents were \n",
      "stranded in homes, and businesses were unable to \n",
      "open without employees. In 2024, several dozen roofs \n",
      "collapsed and Anchorage officials warned more than \n",
      "1,000 commercial property owners that their roofs \n",
      "were at risk due to the heavy snow loads. Impacts \n",
      "from the storm were ongoing for months.\n",
      "ANCHORAGE SNOW\n",
      "Daily snow for the past three \n",
      "winters in Anchorage. The \n",
      "2023-24 season produced 133.3 \n",
      "inches of snow and was 0.2 \n",
      "inches shy of the 2011-12 record. \n",
      "11\n",
      "Loren Holmes, Anchorage Daily News 11\n"
     ]
    }
   ],
   "source": [
    "print(chunks[34].metadata)\n",
    "print(chunks[34].page_content)\n",
    "print(\"\\n\")\n",
    "print(chunks[35].metadata)\n",
    "print(chunks[35].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the document objects, the chunks don't have unique IDs unless you assign them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[35].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this codebase DO assign unique IDs to chunks based on document name, page number, and chunk number on that page. Here we add a unique ID to the chunk metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Alaskas-Changing-Environment-2024.pdf:10:1\n"
     ]
    }
   ],
   "source": [
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "print(chunks_with_ids[35].metadata[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So chunk #35 is from the AK Changing Environment 2024 document, page 2, chunk 3. As mentioned previously, we can use a custom function here to associate as much metadata as we want with this chunk, so that if the LLM uses this chunk of text to create a response to a question, we can provide rich metadata at the same time and point the user back to the exact source of the information. Not just the name of the source document, but the page number, paragraph, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
